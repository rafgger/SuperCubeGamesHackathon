{
  "nodes": [
    {
      "id": "supervisor_0",
      "position": {
        "x": 800,
        "y": 303
      },
      "type": "customNode",
      "data": {
        "id": "supervisor_0",
        "label": "Supervisor",
        "version": 3,
        "name": "supervisor",
        "type": "Supervisor",
        "baseClasses": [
          "Supervisor"
        ],
        "category": "Multi Agents",
        "inputParams": [
          {
            "label": "Supervisor Name",
            "name": "supervisorName",
            "type": "string",
            "placeholder": "Supervisor",
            "default": "Supervisor",
            "id": "supervisor_0-input-supervisorName-string",
            "display": true
          },
          {
            "label": "Supervisor Prompt",
            "name": "supervisorPrompt",
            "type": "string",
            "description": "Prompt must contains {team_members}",
            "rows": 4,
            "default": "You are a supervisor tasked with managing a conversation between the following workers: {team_members}.\nGiven the following user request, respond with the worker to act next.\nEach worker will perform a task and respond with their results and status.\nWhen finished, respond with FINISH.\nSelect strategically to minimize the number of steps taken.",
            "additionalParams": true,
            "id": "supervisor_0-input-supervisorPrompt-string",
            "display": true
          },
          {
            "label": "Summarization",
            "name": "summarization",
            "type": "boolean",
            "description": "Return final output as a summarization of the conversation",
            "optional": true,
            "additionalParams": true,
            "id": "supervisor_0-input-summarization-boolean",
            "display": true
          },
          {
            "label": "Recursion Limit",
            "name": "recursionLimit",
            "type": "number",
            "description": "Maximum number of times a call can recurse. If not provided, defaults to 100.",
            "default": 100,
            "additionalParams": true,
            "id": "supervisor_0-input-recursionLimit-number",
            "display": true
          }
        ],
        "inputAnchors": [
          {
            "label": "Tool Calling Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "description": "Only compatible with models that are capable of function calling: ChatOpenAI, ChatMistral, ChatAnthropic, ChatGoogleGenerativeAI, GroqChat. Best result with GPT-4 model",
            "id": "supervisor_0-input-model-BaseChatModel",
            "display": true
          },
          {
            "label": "Agent Memory",
            "name": "agentMemory",
            "type": "BaseCheckpointSaver",
            "description": "Save the state of the agent",
            "optional": true,
            "id": "supervisor_0-input-agentMemory-BaseCheckpointSaver",
            "display": true
          },
          {
            "label": "Input Moderation",
            "description": "Detect text that could generate harmful output and prevent it from being sent to the language model",
            "name": "inputModeration",
            "type": "Moderation",
            "optional": true,
            "list": true,
            "id": "supervisor_0-input-inputModeration-Moderation",
            "display": true
          }
        ],
        "inputs": {
          "supervisorName": "Supervisor",
          "supervisorPrompt": "You are a supervisor tasked with managing a conversation between the following workers: {team_members}.\nGiven the following user request, respond with the worker to act next.\nEach worker will perform a task and respond with their results and status.\nWhen finished, respond with FINISH.\nSelect strategically to minimize the number of steps taken.\n\nYou start with the planning agent.",
          "model": "{{chatOpenAI_0.data.instance}}",
          "agentMemory": "",
          "summarization": "",
          "recursionLimit": 100,
          "inputModeration": ""
        },
        "outputAnchors": [
          {
            "id": "supervisor_0-output-supervisor-Supervisor",
            "name": "supervisor",
            "label": "Supervisor",
            "description": "",
            "type": "Supervisor"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 489,
      "positionAbsolute": {
        "x": 800,
        "y": 303
      },
      "selected": false,
      "dragging": false
    },
    {
      "id": "chatOpenAI_0",
      "position": {
        "x": 183.7309896058864,
        "y": 214.28397960969676
      },
      "type": "customNode",
      "data": {
        "id": "chatOpenAI_0",
        "label": "ChatOpenAI",
        "version": 8.3,
        "name": "chatOpenAI",
        "type": "ChatOpenAI",
        "baseClasses": [
          "ChatOpenAI",
          "BaseChatOpenAI",
          "BaseChatModel",
          "BaseLanguageModel",
          "Runnable"
        ],
        "category": "Chat Models",
        "description": "Wrapper around OpenAI large language models that use the Chat endpoint",
        "inputParams": [
          {
            "label": "Connect Credential",
            "name": "credential",
            "type": "credential",
            "credentialNames": [
              "openAIApi"
            ],
            "id": "chatOpenAI_0-input-credential-credential",
            "display": true
          },
          {
            "label": "Model Name",
            "name": "modelName",
            "type": "asyncOptions",
            "loadMethod": "listModels",
            "default": "gpt-4o-mini",
            "id": "chatOpenAI_0-input-modelName-asyncOptions",
            "display": true
          },
          {
            "label": "Temperature",
            "name": "temperature",
            "type": "number",
            "step": 0.1,
            "default": 0.9,
            "optional": true,
            "id": "chatOpenAI_0-input-temperature-number",
            "display": true
          },
          {
            "label": "Streaming",
            "name": "streaming",
            "type": "boolean",
            "default": true,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-streaming-boolean",
            "display": true
          },
          {
            "label": "Max Tokens",
            "name": "maxTokens",
            "type": "number",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-maxTokens-number",
            "display": true
          },
          {
            "label": "Top Probability",
            "name": "topP",
            "type": "number",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-topP-number",
            "display": true
          },
          {
            "label": "Frequency Penalty",
            "name": "frequencyPenalty",
            "type": "number",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-frequencyPenalty-number",
            "display": true
          },
          {
            "label": "Presence Penalty",
            "name": "presencePenalty",
            "type": "number",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-presencePenalty-number",
            "display": true
          },
          {
            "label": "Timeout",
            "name": "timeout",
            "type": "number",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-timeout-number",
            "display": true
          },
          {
            "label": "Strict Tool Calling",
            "name": "strictToolCalling",
            "type": "boolean",
            "description": "Whether the model supports the `strict` argument when passing in tools. If not specified, the `strict` argument will not be passed to OpenAI.",
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-strictToolCalling-boolean",
            "display": true
          },
          {
            "label": "Stop Sequence",
            "name": "stopSequence",
            "type": "string",
            "rows": 4,
            "optional": true,
            "description": "List of stop words to use when generating. Use comma to separate multiple stop words.",
            "additionalParams": true,
            "id": "chatOpenAI_0-input-stopSequence-string",
            "display": true
          },
          {
            "label": "BasePath",
            "name": "basepath",
            "type": "string",
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-basepath-string",
            "display": true
          },
          {
            "label": "Proxy Url",
            "name": "proxyUrl",
            "type": "string",
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-proxyUrl-string",
            "display": true
          },
          {
            "label": "BaseOptions",
            "name": "baseOptions",
            "type": "json",
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-baseOptions-json",
            "display": true
          },
          {
            "label": "Allow Image Uploads",
            "name": "allowImageUploads",
            "type": "boolean",
            "description": "Allow image input. Refer to the <a href=\"https://docs.flowiseai.com/using-flowise/uploads#image\" target=\"_blank\">docs</a> for more details.",
            "default": false,
            "optional": true,
            "id": "chatOpenAI_0-input-allowImageUploads-boolean",
            "display": true
          },
          {
            "label": "Image Resolution",
            "description": "This parameter controls the resolution in which the model views the image.",
            "name": "imageResolution",
            "type": "options",
            "options": [
              {
                "label": "Low",
                "name": "low"
              },
              {
                "label": "High",
                "name": "high"
              },
              {
                "label": "Auto",
                "name": "auto"
              }
            ],
            "default": "low",
            "optional": false,
            "show": {
              "allowImageUploads": true
            },
            "id": "chatOpenAI_0-input-imageResolution-options",
            "display": false
          },
          {
            "label": "Reasoning",
            "description": "Whether the model supports reasoning. Only applicable for reasoning models.",
            "name": "reasoning",
            "type": "boolean",
            "default": false,
            "optional": true,
            "additionalParams": true,
            "id": "chatOpenAI_0-input-reasoning-boolean",
            "display": true
          },
          {
            "label": "Reasoning Effort",
            "description": "Constrains effort on reasoning for reasoning models",
            "name": "reasoningEffort",
            "type": "options",
            "options": [
              {
                "label": "Low",
                "name": "low"
              },
              {
                "label": "Medium",
                "name": "medium"
              },
              {
                "label": "High",
                "name": "high"
              }
            ],
            "additionalParams": true,
            "show": {
              "reasoning": true
            },
            "id": "chatOpenAI_0-input-reasoningEffort-options",
            "display": false
          },
          {
            "label": "Reasoning Summary",
            "description": "A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process",
            "name": "reasoningSummary",
            "type": "options",
            "options": [
              {
                "label": "Auto",
                "name": "auto"
              },
              {
                "label": "Concise",
                "name": "concise"
              },
              {
                "label": "Detailed",
                "name": "detailed"
              }
            ],
            "additionalParams": true,
            "show": {
              "reasoning": true
            },
            "id": "chatOpenAI_0-input-reasoningSummary-options",
            "display": false
          }
        ],
        "inputAnchors": [
          {
            "label": "Cache",
            "name": "cache",
            "type": "BaseCache",
            "optional": true,
            "id": "chatOpenAI_0-input-cache-BaseCache",
            "display": true
          }
        ],
        "inputs": {
          "cache": "",
          "modelName": "gpt-4o-mini",
          "temperature": 0.9,
          "streaming": true,
          "maxTokens": "1200",
          "topP": "",
          "frequencyPenalty": "",
          "presencePenalty": "",
          "timeout": "",
          "strictToolCalling": "",
          "stopSequence": "",
          "basepath": "",
          "proxyUrl": "",
          "baseOptions": "",
          "allowImageUploads": "",
          "imageResolution": "low",
          "reasoning": "",
          "reasoningEffort": "",
          "reasoningSummary": ""
        },
        "outputAnchors": [
          {
            "id": "chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable",
            "name": "chatOpenAI",
            "label": "ChatOpenAI",
            "description": "Wrapper around OpenAI large language models that use the Chat endpoint",
            "type": "ChatOpenAI | BaseChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 676,
      "selected": false,
      "positionAbsolute": {
        "x": 183.7309896058864,
        "y": 214.28397960969676
      },
      "dragging": false
    },
    {
      "id": "worker_0",
      "position": {
        "x": 1325.4732646038237,
        "y": 426.6627442260616
      },
      "type": "customNode",
      "data": {
        "id": "worker_0",
        "label": "Worker",
        "version": 2,
        "name": "worker",
        "type": "Worker",
        "hideOutput": true,
        "baseClasses": [
          "Worker"
        ],
        "category": "Multi Agents",
        "inputParams": [
          {
            "label": "Worker Name",
            "name": "workerName",
            "type": "string",
            "placeholder": "Worker",
            "id": "worker_0-input-workerName-string",
            "display": true
          },
          {
            "label": "Worker Prompt",
            "name": "workerPrompt",
            "type": "string",
            "rows": 4,
            "default": "You are a research assistant who can search for up-to-date info using search engine.",
            "id": "worker_0-input-workerPrompt-string",
            "display": true
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "id": "worker_0-input-promptValues-json",
            "display": true
          },
          {
            "label": "Max Iterations",
            "name": "maxIterations",
            "type": "number",
            "optional": true,
            "id": "worker_0-input-maxIterations-number",
            "display": true
          }
        ],
        "inputAnchors": [
          {
            "label": "Tools",
            "name": "tools",
            "type": "Tool",
            "list": true,
            "optional": true,
            "id": "worker_0-input-tools-Tool",
            "display": true
          },
          {
            "label": "Supervisor",
            "name": "supervisor",
            "type": "Supervisor",
            "id": "worker_0-input-supervisor-Supervisor",
            "display": true
          },
          {
            "label": "Tool Calling Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "optional": true,
            "description": "Only compatible with models that are capable of function calling: ChatOpenAI, ChatMistral, ChatAnthropic, ChatGoogleGenerativeAI, ChatVertexAI, GroqChat. If not specified, supervisor's model will be used",
            "id": "worker_0-input-model-BaseChatModel",
            "display": true
          }
        ],
        "inputs": {
          "workerName": "Trip planner",
          "workerPrompt": "You are a Trip Planner agent.\n\nYour job is to convert user travel requests into structured flight search parameters.\nIf required information is missing, ask a clarification question instead of returning JSON.\n\nRequired fields:\n- origin (IATA code)\n- destination (IATA code)\n- date_from or date_range\n\nWhen ready, return VALID JSON ONLY.\nDo not invent values.",
          "tools": [
            "{{customMCP_0.data.instance}}"
          ],
          "supervisor": "{{supervisor_0.data.instance}}",
          "model": "",
          "promptValues": "",
          "maxIterations": ""
        },
        "outputAnchors": [],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 728,
      "positionAbsolute": {
        "x": 1325.4732646038237,
        "y": 426.6627442260616
      },
      "selected": false,
      "dragging": false
    },
    {
      "id": "worker_1",
      "position": {
        "x": 1840.957425823257,
        "y": 486.87900516851795
      },
      "type": "customNode",
      "data": {
        "id": "worker_1",
        "label": "Worker",
        "version": 2,
        "name": "worker",
        "type": "Worker",
        "hideOutput": true,
        "baseClasses": [
          "Worker"
        ],
        "category": "Multi Agents",
        "inputParams": [
          {
            "label": "Worker Name",
            "name": "workerName",
            "type": "string",
            "placeholder": "Worker",
            "id": "worker_1-input-workerName-string",
            "display": true
          },
          {
            "label": "Worker Prompt",
            "name": "workerPrompt",
            "type": "string",
            "rows": 4,
            "default": "You are a research assistant who can search for up-to-date info using search engine.",
            "id": "worker_1-input-workerPrompt-string",
            "display": true
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "id": "worker_1-input-promptValues-json",
            "display": true
          },
          {
            "label": "Max Iterations",
            "name": "maxIterations",
            "type": "number",
            "optional": true,
            "id": "worker_1-input-maxIterations-number",
            "display": true
          }
        ],
        "inputAnchors": [
          {
            "label": "Tools",
            "name": "tools",
            "type": "Tool",
            "list": true,
            "optional": true,
            "id": "worker_1-input-tools-Tool",
            "display": true
          },
          {
            "label": "Supervisor",
            "name": "supervisor",
            "type": "Supervisor",
            "id": "worker_1-input-supervisor-Supervisor",
            "display": true
          },
          {
            "label": "Tool Calling Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "optional": true,
            "description": "Only compatible with models that are capable of function calling: ChatOpenAI, ChatMistral, ChatAnthropic, ChatGoogleGenerativeAI, ChatVertexAI, GroqChat. If not specified, supervisor's model will be used",
            "id": "worker_1-input-model-BaseChatModel",
            "display": true
          }
        ],
        "inputs": {
          "workerName": "",
          "workerPrompt": "You are a research assistant who can search for up-to-date info using search engine.",
          "tools": [],
          "supervisor": "",
          "model": "",
          "promptValues": "",
          "maxIterations": ""
        },
        "outputAnchors": [],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 728,
      "positionAbsolute": {
        "x": 1840.957425823257,
        "y": 486.87900516851795
      },
      "selected": false,
      "dragging": false
    },
    {
      "id": "customMCP_0",
      "position": {
        "x": 810.3069928693616,
        "y": 888.1281608627404
      },
      "type": "customNode",
      "data": {
        "id": "customMCP_0",
        "label": "Custom MCP",
        "version": 1.1,
        "name": "customMCP",
        "type": "Custom MCP Tool",
        "baseClasses": [
          "Tool"
        ],
        "category": "Tools (MCP)",
        "description": "Custom MCP Config",
        "inputParams": [
          {
            "label": "MCP Server Config",
            "name": "mcpServerConfig",
            "type": "code",
            "hideCodeExecute": true,
            "hint": {
              "label": "How to use",
              "value": "\nYou can use variables in the MCP Server Config with double curly braces `{{ }}` and prefix `$vars.<variableName>`. \n\nFor example, you have a variable called \"var1\":\n```json\n{\n    \"command\": \"docker\",\n    \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\", \"API_TOKEN\"\n    ],\n    \"env\": {\n        \"API_TOKEN\": \"{{$vars.var1}}\"\n    }\n}\n```\n\nFor example, when using SSE, you can use the variable \"var1\" in the headers:\n```json\n{\n    \"url\": \"https://api.example.com/endpoint/sse\",\n    \"headers\": {\n        \"Authorization\": \"Bearer {{$vars.var1}}\"\n    }\n}\n```\n"
            },
            "placeholder": "{\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/allowed/files\"]\n}",
            "id": "customMCP_0-input-mcpServerConfig-code",
            "display": true
          },
          {
            "label": "Available Actions",
            "name": "mcpActions",
            "type": "asyncMultiOptions",
            "loadMethod": "listActions",
            "refresh": true,
            "id": "customMCP_0-input-mcpActions-asyncMultiOptions",
            "display": true
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "mcpServerConfig": "{\n  \"url\": \"https://mcp.kiwi.com\",\n  \"headers\": {}\n}",
          "mcpActions": "[\"search-flight\"]"
        },
        "outputAnchors": [
          {
            "id": "customMCP_0-output-customMCP-Tool",
            "name": "customMCP",
            "label": "Custom MCP Tool",
            "description": "Custom MCP Config",
            "type": "Tool"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 577,
      "selected": false,
      "positionAbsolute": {
        "x": 810.3069928693616,
        "y": 888.1281608627404
      },
      "dragging": false
    }
  ],
  "edges": [
    {
      "source": "chatOpenAI_0",
      "sourceHandle": "chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable",
      "target": "supervisor_0",
      "targetHandle": "supervisor_0-input-model-BaseChatModel",
      "type": "buttonedge",
      "id": "chatOpenAI_0-chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable-supervisor_0-supervisor_0-input-model-BaseChatModel"
    },
    {
      "source": "supervisor_0",
      "sourceHandle": "supervisor_0-output-supervisor-Supervisor",
      "target": "worker_0",
      "targetHandle": "worker_0-input-supervisor-Supervisor",
      "type": "buttonedge",
      "id": "supervisor_0-supervisor_0-output-supervisor-Supervisor-worker_0-worker_0-input-supervisor-Supervisor"
    },
    {
      "source": "customMCP_0",
      "sourceHandle": "customMCP_0-output-customMCP-Tool",
      "target": "worker_0",
      "targetHandle": "worker_0-input-tools-Tool",
      "type": "buttonedge",
      "id": "customMCP_0-customMCP_0-output-customMCP-Tool-worker_0-worker_0-input-tools-Tool"
    }
  ]
}